{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.types import IntegerType, DateType, TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "spark = (ps.sql.SparkSession.builder \n",
    "        .master(\"local[4]\") \n",
    "        .appName(\"sparkSQL exercise\") \n",
    "        .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7e5b74eb1d72:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sparkSQL exercise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=sparkSQL exercise>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(\"uk100.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- rank: string (nullable = true)\n",
      " |-- last_week_rank: string (nullable = true)\n",
      " |-- hmm: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- peak_rank: string (nullable = true)\n",
      " |-- weeks_on_chart: string (nullable = true)\n",
      " |-- week_of: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------+--------+--------------------+--------------------+--------------------+---------+--------------+----------------+\n",
      "|_c0|rank|last_week_rank|     hmm|               title|              artist|               label|peak_rank|weeks_on_chart|         week_of|\n",
      "+---+----+--------------+--------+--------------------+--------------------+--------------------+---------+--------------+----------------+\n",
      "|  0|   1|            11|        |               RIVER|      ELLIE GOULDING|             POLYDOR|        1|             5|December 27 2019|\n",
      "|  1|   2|             8|        |ALL I WANT FOR CH...|        MARIAH CAREY|            COLUMBIA|        2|            99|December 27 2019|\n",
      "|  2|   3|             5|        |      LAST CHRISTMAS|                WHAM|                 RCA|        2|            64|December 27 2019|\n",
      "|  3|   4|            14|        |FAIRYTALE OF NEW ...|POGUES FT KIRSTY ...|         WARNER BROS|        2|            99|December 27 2019|\n",
      "|  4|   5|             2|        |              OWN IT|STORMZY/ED SHEERA...|      ATLANTIC/MERKY|        2|             5|December 27 2019|\n",
      "|  5|   6|            16|        |MERRY CHRISTMAS E...|     SHAKIN' STEVENS|                 RCA|        1|            71|December 27 2019|\n",
      "|  6|   7|            17|        |DO THEY KNOW IT'S...|            BAND AID|             MERCURY|        1|            80|December 27 2019|\n",
      "|  7|   8|            19|        | STEP INTO CHRISTMAS|          ELTON JOHN|             MERCURY|        8|            38|December 27 2019|\n",
      "|  8|   9|            34|        |HAPPY CHRISTMAS (...|         JOHN LEGEND|            COLUMBIA|        9|             3|December 27 2019|\n",
      "|  9|  10|            29|        |I WISH IT COULD B...|             WIZZARD|                 EMI|       10|            57|December 27 2019|\n",
      "| 10|  11|            24|        |IT'S BEGINNING TO...|       MICHAEL BUBLE|             REPRISE|        7|            33|December 27 2019|\n",
      "| 11|  12|             3|        |       BEFORE YOU GO|       LEWIS CAPALDI|                 EMI|        2|             6|December 27 2019|\n",
      "| 12|  13|            28|        |       SANTA TELL ME|       ARIANA GRANDE|    REPUBLIC RECORDS|       13|            18|December 27 2019|\n",
      "| 13|  14|             4|        |     DON'T START NOW|            DUA LIPA|      WARNER RECORDS|        2|             8|December 27 2019|\n",
      "| 14|  15|            25|        |      ONE MORE SLEEP|         LEONA LEWIS|          SYCO MUSIC|        3|            23|December 27 2019|\n",
      "| 15|  16|             7|        |             ROXANNE|      ARIZONA ZERVAS|          SONY MUSIC|        5|             8|December 27 2019|\n",
      "| 16|  17|            36|        |SANTA'S COMING FO...|                 SIA|ATLANTIC/MONKEY P...|       17|            12|December 27 2019|\n",
      "| 17|  18|            32|        |ROCKIN' AROUND TH...|          BRENDA LEE|                 MCA|        6|            47|December 27 2019|\n",
      "| 18|  19|            31|        |MERRY XMAS EVERYBODY|               SLADE|                UMTV|        1|            98|December 27 2019|\n",
      "| 19|  20|            10|        |        DANCE MONKEY|           TONES & I|          PARLOPHONE|        1|            21|December 27 2019|\n",
      "+---+----+--------------+--------+--------------------+--------------------+--------------------+---------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''SELECT *\n",
    "            FROM test\n",
    "            LIMIT 30\n",
    "            ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('_c0')\n",
    "df = df.drop('hmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    df = df.withColumn(col, f.lower(f.col(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"rank\", df[\"rank\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"peak_rank\", df[\"peak_rank\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"weeks_on_chart\", df[\"weeks_on_chart\"].cast(IntegerType()))\n",
    "#df = df.withColumn(\"week_of\", df[\"week_of\"].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date(x):\n",
    "    return datetime.strptime(x, '%B %d %Y')\n",
    "hmm = f.udf(lambda y: to_date(y), DateType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"week_of\", hmm('week_of'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rank: integer (nullable = true)\n",
      " |-- last_week_rank: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- peak_rank: integer (nullable = true)\n",
      " |-- weeks_on_chart: integer (nullable = true)\n",
      " |-- week_of: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   26000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''SELECT COUNT(*)\n",
    "            FROM test\n",
    "            ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+--------------------+--------------------+--------------------+---------+--------------+----------+\n",
      "|rank|last_week_rank|               title|              artist|               label|peak_rank|weeks_on_chart|   week_of|\n",
      "+----+--------------+--------------------+--------------------+--------------------+---------+--------------+----------+\n",
      "|   1|            11|               river|      ellie goulding|             polydor|        1|             5|2019-12-27|\n",
      "|   2|             8|all i want for ch...|        mariah carey|            columbia|        2|            99|2019-12-27|\n",
      "|   3|             5|      last christmas|                wham|                 rca|        2|            64|2019-12-27|\n",
      "|   4|            14|fairytale of new ...|pogues ft kirsty ...|         warner bros|        2|            99|2019-12-27|\n",
      "|   5|             2|              own it|stormzy/ed sheera...|      atlantic/merky|        2|             5|2019-12-27|\n",
      "|   6|            16|merry christmas e...|     shakin' stevens|                 rca|        1|            71|2019-12-27|\n",
      "|   7|            17|do they know it's...|            band aid|             mercury|        1|            80|2019-12-27|\n",
      "|   8|            19| step into christmas|          elton john|             mercury|        8|            38|2019-12-27|\n",
      "|   9|            34|happy christmas (...|         john legend|            columbia|        9|             3|2019-12-27|\n",
      "|  10|            29|i wish it could b...|             wizzard|                 emi|       10|            57|2019-12-27|\n",
      "|  11|            24|it's beginning to...|       michael buble|             reprise|        7|            33|2019-12-27|\n",
      "|  12|             3|       before you go|       lewis capaldi|                 emi|        2|             6|2019-12-27|\n",
      "|  13|            28|       santa tell me|       ariana grande|    republic records|       13|            18|2019-12-27|\n",
      "|  14|             4|     don't start now|            dua lipa|      warner records|        2|             8|2019-12-27|\n",
      "|  15|            25|      one more sleep|         leona lewis|          syco music|        3|            23|2019-12-27|\n",
      "|  16|             7|             roxanne|      arizona zervas|          sony music|        5|             8|2019-12-27|\n",
      "|  17|            36|santa's coming fo...|                 sia|atlantic/monkey p...|       17|            12|2019-12-27|\n",
      "|  18|            32|rockin' around th...|          brenda lee|                 mca|        6|            47|2019-12-27|\n",
      "|  19|            31|merry xmas everybody|               slade|                umtv|        1|            98|2019-12-27|\n",
      "|  20|            10|        dance monkey|           tones & i|          parlophone|        1|            21|2019-12-27|\n",
      "+----+--------------+--------------------+--------------------+--------------------+---------+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''SELECT *\n",
    "            FROM test\n",
    "            LIMIT 30\n",
    "            ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+-----------------------------------+\n",
      "|title                                   |round(avg(CAST(rank AS BIGINT)), 2)|\n",
      "+----------------------------------------+-----------------------------------+\n",
      "|last christmas                          |25.3                               |\n",
      "|all i want for christmas is you         |26.82                              |\n",
      "|happy christmas (war is over)           |28.0                               |\n",
      "|merry christmas everyone                |34.46                              |\n",
      "|do they know it's christmas             |35.7                               |\n",
      "|i wish it could be christmas everyday   |36.0                               |\n",
      "|rockin' around the christmas tree       |37.05                              |\n",
      "|step into christmas                     |38.84                              |\n",
      "|driving home for christmas              |39.37                              |\n",
      "|santa tell me                           |41.0                               |\n",
      "|merry xmas everybody                    |45.5                               |\n",
      "|happy xmas (war is over)                |51.06                              |\n",
      "|cozy little christmas                   |51.43                              |\n",
      "|santa's coming for us                   |51.58                              |\n",
      "|when christmas comes around             |51.83                              |\n",
      "|wonderful christmastime                 |56.17                              |\n",
      "|christmas (baby please come home)       |61.5                               |\n",
      "|christmas lights                        |63.63                              |\n",
      "|holly jolly christmas                   |63.85                              |\n",
      "|white christmas                         |64.28                              |\n",
      "|like it's christmas                     |68.0                               |\n",
      "|i believe in father christmas           |69.0                               |\n",
      "|the christmas song                      |69.4                               |\n",
      "|blue christmas                          |70.25                              |\n",
      "|santa claus is comin' to town           |71.67                              |\n",
      "|santa baby                              |71.82                              |\n",
      "|all i want (for christmas)              |73.0                               |\n",
      "|santa claus is coming to town           |75.67                              |\n",
      "|lonely this christmas                   |77.5                               |\n",
      "|christmas wrapping                      |78.6                               |\n",
      "|christmas time (don't let the bells end)|79.9                               |\n",
      "|christmas tree farm                     |85.0                               |\n",
      "|thank god it's christmas                |86.0                               |\n",
      "|have yourself a merry little christmas  |86.4                               |\n",
      "|you make it feel like christmas         |88.2                               |\n",
      "|run rudolph run                         |88.5                               |\n",
      "+----------------------------------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT DISTINCT title, ROUND(AVG(rank), 2)\n",
    "            FROM test\n",
    "            WHERE title like \"%christmas%\"\n",
    "                OR title like \"%merry%\"\n",
    "                OR title like \"%xmas%\"\n",
    "                OR title like \"%santa%\"\n",
    "                OR title like \"%rudolph%\"\n",
    "                OR title like \"%reindeer%\"\n",
    "            GROUP BY 1\n",
    "            ORDER BY 2\n",
    "            ''')\n",
    "result.show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT title)|\n",
      "+---------------------+\n",
      "|                 2416|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT COUNT(DISTINCT title)\n",
    "            FROM test\n",
    "            ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT title)|\n",
      "+---------------------+\n",
      "|                   77|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT COUNT(DISTINCT title)\n",
    "            FROM test\n",
    "            WHERE rank == 1''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------+\n",
      "|               title|num_weeks|min(week_of)|\n",
      "+--------------------+---------+------------+\n",
      "|           one dance|       15|  2016-04-15|\n",
      "|        shape of you|       14|  2017-01-13|\n",
      "|        dance monkey|       11|  2019-10-04|\n",
      "|   despacito (remix)|       11|  2017-05-12|\n",
      "|          god's plan|        9|  2018-01-26|\n",
      "|            rockabye|        9|  2016-11-11|\n",
      "|            one kiss|        8|  2018-04-20|\n",
      "|        i don't care|        8|  2019-05-17|\n",
      "|   someone you loved|        7|  2019-03-01|\n",
      "|       love yourself|        6|  2015-12-04|\n",
      "|        thank u next|        6|  2018-11-09|\n",
      "|             perfect|        6|  2017-12-08|\n",
      "|            promises|        6|  2018-09-07|\n",
      "|            senorita|        6|  2019-07-12|\n",
      "|          cold water|        5|  2016-07-29|\n",
      "|              havana|        5|  2017-11-03|\n",
      "|take me back to l...|        5|  2019-08-30|\n",
      "|             7 years|        5|  2016-02-12|\n",
      "|    what do you mean|        5|  2015-09-04|\n",
      "|i took a pill in ...|        4|  2016-03-18|\n",
      "+--------------------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT DISTINCT title, COUNT(*) as num_weeks, MIN(week_of)\n",
    "            FROM test\n",
    "            WHERE rank == 1\n",
    "            GROUP BY 1\n",
    "            ORDER BY 2 DESC\n",
    "            ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+------------+\n",
      "|              title|num_weeks|min(week_of)|\n",
      "+-------------------+---------+------------+\n",
      "|       shape of you|       97|  2017-01-13|\n",
      "|  thinking out loud|       90|  2015-01-11|\n",
      "|            perfect|       81|  2017-12-08|\n",
      "|            shotgun|       74|  2018-06-29|\n",
      "|        uptown funk|       72|  2015-01-11|\n",
      "|  despacito (remix)|       71|  2017-05-12|\n",
      "|              sorry|       65|  2015-11-20|\n",
      "|          one dance|       64|  2016-04-15|\n",
      "|   what do you mean|       62|  2015-09-04|\n",
      "|      love yourself|       62|  2015-12-04|\n",
      "|          new rules|       61|  2017-08-18|\n",
      "|               king|       59|  2015-03-08|\n",
      "|         these days|       59|  2015-01-11|\n",
      "|love me like you do|       57|  2015-02-08|\n",
      "|            shallow|       57|  2018-10-26|\n",
      "|           one kiss|       52|  2018-04-20|\n",
      "|           stitches|       51|  2016-01-22|\n",
      "|       hold my hand|       51|  2015-03-29|\n",
      "|            7 years|       49|  2016-02-12|\n",
      "|              hello|       49|  2015-10-30|\n",
      "+-------------------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT DISTINCT title, COUNT(*) as num_weeks, MIN(week_of)\n",
    "            FROM test\n",
    "            WHERE peak_rank == 1\n",
    "            GROUP BY 1\n",
    "            ORDER BY 2 DESC\n",
    "            ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|    avg(num_weeks)|\n",
      "+------------------+\n",
      "|26.708661417322833|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT AVG(num_weeks)\n",
    "            FROM (SELECT DISTINCT title, COUNT(*) as num_weeks, MIN(week_of)\n",
    "                    FROM test\n",
    "                    WHERE peak_rank == 1\n",
    "                    GROUP BY 1\n",
    "                    ORDER BY 2 DESC)\n",
    "            ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|        avg(rank)|max(weeks_on_chart)|\n",
      "+-----------------+-------------------+\n",
      "|42.51546391752577|                 97|\n",
      "+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT AVG(rank), MAX(weeks_on_chart)\n",
    "            FROM test\n",
    "            WHERE title = \"shape of you\"\n",
    "            ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+\n",
      "|              artist|               title|count(1)|\n",
      "+--------------------+--------------------+--------+\n",
      "|          ed sheeran|             perfect|     110|\n",
      "|             killers|       mr brightside|     105|\n",
      "|          ed sheeran|        shape of you|      97|\n",
      "|          ed sheeran|   thinking out loud|      90|\n",
      "|         george ezra|             shotgun|      87|\n",
      "|                 sia|          chandelier|      87|\n",
      "|major lazer ft mo...|             lean on|      79|\n",
      "|           james bay|           let it go|      77|\n",
      "|settle/greatest s...|          this is me|      75|\n",
      "|          ed sheeran|          photograph|      73|\n",
      "|       walk the moon|     shut up & dance|      72|\n",
      "|mark ronson ft br...|         uptown funk|      72|\n",
      "|   justin timberlake|can't stop the fe...|      71|\n",
      "|luis fonsi/daddy ...|   despacito (remix)|      71|\n",
      "|           james bay| hold back the river|      68|\n",
      "|       justin bieber|               sorry|      68|\n",
      "|        shawn mendes|            stitches|      67|\n",
      "|              hozier|   take me to church|      66|\n",
      "|                 sia|       cheap thrills|      66|\n",
      "|            dua lipa|           new rules|      66|\n",
      "+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT artist, title, COUNT(*)\n",
    "            FROM test\n",
    "            GROUP BY artist, title\n",
    "            ORDER BY 3 DESC\n",
    "            ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+--------+\n",
      "|       artist|  title|count(1)|\n",
      "+-------------+-------+--------+\n",
      "|one direction|perfect|      21|\n",
      "|   ed sheeran|perfect|     110|\n",
      "+-------------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT artist, title, count(*)\n",
    "            FROM test\n",
    "            WHERE title = \"perfect\"\n",
    "            GROUP BY artist, title\n",
    "            ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----+--------+\n",
      "|              artist|               title|               label|count|avg_rank|\n",
      "+--------------------+--------------------+--------------------+-----+--------+\n",
      "|          ed sheeran|             perfect|              asylum|  110|   48.09|\n",
      "|             killers|       mr brightside|             mercury|  105|   84.86|\n",
      "|          ed sheeran|        shape of you|              asylum|   97|   42.52|\n",
      "|          ed sheeran|   thinking out loud|              asylum|   90|   56.27|\n",
      "|                 sia|          chandelier|   monkey puzzle/rca|   87|   64.46|\n",
      "|         george ezra|             shotgun|            columbia|   87|   35.99|\n",
      "|major lazer ft mo...|             lean on|       because music|   79|   45.92|\n",
      "|           james bay|           let it go|              virgin|   77|   65.03|\n",
      "|settle/greatest s...|          this is me|            atlantic|   75|   40.84|\n",
      "|          ed sheeran|          photograph|              asylum|   73|    54.6|\n",
      "|mark ronson ft br...|         uptown funk|            columbia|   72|   43.81|\n",
      "|       walk the moon|     shut up & dance|                 rca|   72|   54.63|\n",
      "|luis fonsi/daddy ...|   despacito (remix)|def jam/rbmg/repu...|   71|   51.11|\n",
      "|   justin timberlake|can't stop the fe...|                 rca|   71|   46.94|\n",
      "|       justin bieber|               sorry|             def jam|   68|   41.43|\n",
      "|           james bay| hold back the river|              virgin|   68|   47.06|\n",
      "|        shawn mendes|            stitches|                 emi|   67|   47.63|\n",
      "|            dua lipa|           new rules|         warner bros|   66|   43.92|\n",
      "|              hozier|   take me to church|          ruby works|   66|   46.67|\n",
      "|                 sia|       cheap thrills|   monkey puzzle/rca|   66|   39.12|\n",
      "+--------------------+--------------------+--------------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT artist, title, label, count(*) as count, ROUND(AVG(rank),2) AS avg_rank\n",
    "            FROM test\n",
    "            GROUP BY artist, title, label\n",
    "            ORDER BY count DESC\n",
    "            ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|               label|count(1)|\n",
      "+--------------------+--------+\n",
      "|            atlantic|    2055|\n",
      "|            columbia|    1613|\n",
      "|          interscope|    1493|\n",
      "|             polydor|    1220|\n",
      "|                 rca|    1099|\n",
      "|              virgin|     924|\n",
      "|              asylum|     911|\n",
      "|                 emi|     830|\n",
      "|          syco music|     821|\n",
      "|    republic records|     817|\n",
      "|          parlophone|     791|\n",
      "|         warner bros|     653|\n",
      "|              island|     650|\n",
      "|cash money/republ...|     650|\n",
      "|             capitol|     612|\n",
      "|             def jam|     519|\n",
      "|                epic|     471|\n",
      "|   ministry of sound|     466|\n",
      "|            positiva|     413|\n",
      "|          relentless|     360|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT label, COUNT(*)\n",
    "            FROM test\n",
    "            GROUP BY 1\n",
    "            ORDER BY 2 DESC\n",
    "            ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT label)|\n",
      "+---------------------+\n",
      "|                  408|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT COUNT(DISTINCT label)\n",
    "            FROM test\n",
    "            ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+\n",
      "|               label|num_songs|avg_rank|\n",
      "+--------------------+---------+--------+\n",
      "|        castle music|        1|    13.0|\n",
      "|            emubands|        1|    15.0|\n",
      "|      asylum/def jam|        1|   17.14|\n",
      "|interscope/republ...|        1|   17.56|\n",
      "|cactus jack/epic/...|        1|    22.6|\n",
      "|     asylum/columbia|        1|    25.0|\n",
      "|      emi/syco music|        1|   25.46|\n",
      "|                  zy|        1|    26.5|\n",
      "|       columbia/kygo|        1|   26.72|\n",
      "|black butter/def jam|        2|   26.89|\n",
      "|       perfect havoc|        1|   27.86|\n",
      "|def jam/polydor/r...|        1|   27.88|\n",
      "|             liv'n'g|        1|    28.0|\n",
      "|polydor/rca/repub...|        1|   28.44|\n",
      "|  def jam/parlophone|        1|   29.06|\n",
      "|      merky/atlantic|        1|   29.25|\n",
      "|        fuller beans|        1|    30.0|\n",
      "|          dave clark|        1|    31.0|\n",
      "|   james grant music|        1|    31.0|\n",
      "|black butter/dave...|        1|    31.2|\n",
      "+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT label, COUNT(DISTINCT title) AS num_songs, ROUND(AVG(rank),2) AS avg_rank\n",
    "            FROM test\n",
    "            GROUP BY 1\n",
    "            ORDER BY 3\n",
    "            ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+\n",
      "|               label|num_songs|avg_rank|\n",
      "+--------------------+---------+--------+\n",
      "|          interscope|      171|   51.51|\n",
      "|                 rca|      136|   53.36|\n",
      "|            columbia|      134|   48.64|\n",
      "|            atlantic|      129|   47.02|\n",
      "|              virgin|      117|   54.65|\n",
      "|             polydor|      110|   50.31|\n",
      "|              island|       90|   54.77|\n",
      "|          syco music|       80|   46.77|\n",
      "|          parlophone|       79|   51.33|\n",
      "|cash money/republ...|       78|   50.38|\n",
      "|                 emi|       76|   47.93|\n",
      "|             def jam|       69|    54.3|\n",
      "|         warner bros|       60|   47.58|\n",
      "|    republic records|       59|   43.67|\n",
      "|             capitol|       48|   48.49|\n",
      "|              asylum|       43|   48.22|\n",
      "|                epic|       42|   53.03|\n",
      "|   ministry of sound|       34|   46.21|\n",
      "| republic records/xo|       33|   47.52|\n",
      "|          relentless|       26|   54.46|\n",
      "+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT label, COUNT(DISTINCT title) AS num_songs, ROUND(AVG(rank),2) AS avg_rank\n",
    "            FROM test\n",
    "            GROUP BY 1\n",
    "            ORDER BY 2 DESC\n",
    "            ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|max(week_of)|min(week_of)|\n",
      "+------------+------------+\n",
      "|  2019-12-27|  2015-01-11|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT MAX(week_of), MIN(week_of)\n",
    "            FROM test\n",
    "            ''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+\n",
      "|        artist|count(1)|avg_rank|\n",
      "+--------------+--------+--------+\n",
      "|    ed sheeran|     706|   49.83|\n",
      "|         drake|     352|    49.4|\n",
      "| justin bieber|     280|   47.48|\n",
      "| ariana grande|     269|   39.33|\n",
      "|   george ezra|     263|   48.57|\n",
      "|           sia|     244|   55.59|\n",
      "|    little mix|     244|   45.81|\n",
      "|        weeknd|     228|   46.48|\n",
      "|     sam smith|     216|   48.84|\n",
      "|  shawn mendes|     213|   42.92|\n",
      "|   jess glynne|     207|   44.33|\n",
      "| years & years|     197|   50.62|\n",
      "|  taylor swift|     189|   54.21|\n",
      "|      dua lipa|     172|   41.03|\n",
      "|     james bay|     169|   58.82|\n",
      "| lewis capaldi|     158|   29.18|\n",
      "|      coldplay|     157|   53.01|\n",
      "|       stormzy|     142|    49.3|\n",
      "|ellie goulding|     142|   45.52|\n",
      "|  zara larsson|     142|   43.61|\n",
      "+--------------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT DISTINCT artist, COUNT(*), ROUND(AVG(rank),2) AS avg_rank\n",
    "            FROM test\n",
    "            GROUP BY 1\n",
    "            ORDER BY 2 DESC''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+----+\n",
      "|title                         |count(1)|peak|\n",
      "+------------------------------+--------+----+\n",
      "|perfect                       |110     |1   |\n",
      "|shape of you                  |97      |1   |\n",
      "|thinking out loud             |90      |3   |\n",
      "|photograph                    |73      |15  |\n",
      "|castle on the hill            |53      |2   |\n",
      "|bloodstream                   |38      |2   |\n",
      "|galway girl                   |31      |2   |\n",
      "|sing                          |23      |29  |\n",
      "|supermarket flowers           |21      |8   |\n",
      "|don't                         |20      |20  |\n",
      "|happier                       |18      |6   |\n",
      "|what do i know                |17      |9   |\n",
      "|i see fire                    |15      |54  |\n",
      "|new man                       |14      |5   |\n",
      "|barcelona                     |13      |12  |\n",
      "|dive                          |13      |8   |\n",
      "|how would you feel (paean)    |12      |2   |\n",
      "|nancy mulligan                |10      |13  |\n",
      "|hearts don't break around here|8       |15  |\n",
      "|bibia be ye ye                |8       |18  |\n",
      "|eraser                        |8       |14  |\n",
      "|save myself                   |6       |19  |\n",
      "|the a team                    |5       |52  |\n",
      "|give me love                  |1       |94  |\n",
      "|i will take you home          |1       |69  |\n",
      "|one                           |1       |94  |\n",
      "+------------------------------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT title, COUNT(*), MIN(rank) as peak\n",
    "            FROM test\n",
    "            WHERE artist = \"ed sheeran\"\n",
    "            GROUP BY 1\n",
    "            ORDER BY 2 DESC\n",
    "            ''').show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o494.showString.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(title#116, 200)\n+- *(1) HashAggregate(keys=[title#116], functions=[partial_count(1), partial_min(rank#170)], output=[title#116, count#945L, min#946])\n   +- *(1) Project [cast(lower(rank#11) as int) AS rank#170, lower(title#14) AS title#116]\n      +- *(1) Filter ((lower(artist#15) = ed sheeran) && (min(cast(lower(rank#11) as int)) < 6))\n         +- *(1) FileScan csv [rank#11,title#14,artist#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/uk100.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<rank:string,title:string,artist:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.UnsupportedOperationException: Cannot evaluate expression: min(cast(lower(input[0, string, true]) as int))\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable$class.doGenCode(Expression.scala:261)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression.doGenCode(interfaces.scala:87)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.nullSafeCodeGen(Expression.scala:525)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.defineCodeGen(Expression.scala:508)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryComparison.doGenCode(predicates.scala:561)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)\n\tat org.apache.spark.sql.execution.FilterExec.org$apache$spark$sql$execution$FilterExec$$genPredicate$1(basicPhysicalOperators.scala:139)\n\tat org.apache.spark.sql.execution.FilterExec$$anonfun$13.apply(basicPhysicalOperators.scala:179)\n\tat org.apache.spark.sql.execution.FilterExec$$anonfun$13.apply(basicPhysicalOperators.scala:163)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:163)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.produceRows(ColumnarBatchScan.scala:172)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:85)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:125)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:654)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:166)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:40)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 39 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-0556d99c76e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mGROUP\u001b[0m \u001b[0mBY\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mORDER\u001b[0m \u001b[0mBY\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0mDESC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             ''').show()\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o494.showString.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(title#116, 200)\n+- *(1) HashAggregate(keys=[title#116], functions=[partial_count(1), partial_min(rank#170)], output=[title#116, count#945L, min#946])\n   +- *(1) Project [cast(lower(rank#11) as int) AS rank#170, lower(title#14) AS title#116]\n      +- *(1) Filter ((lower(artist#15) = ed sheeran) && (min(cast(lower(rank#11) as int)) < 6))\n         +- *(1) FileScan csv [rank#11,title#14,artist#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/uk100.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<rank:string,title:string,artist:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.UnsupportedOperationException: Cannot evaluate expression: min(cast(lower(input[0, string, true]) as int))\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable$class.doGenCode(Expression.scala:261)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression.doGenCode(interfaces.scala:87)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.nullSafeCodeGen(Expression.scala:525)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.defineCodeGen(Expression.scala:508)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryComparison.doGenCode(predicates.scala:561)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)\n\tat org.apache.spark.sql.execution.FilterExec.org$apache$spark$sql$execution$FilterExec$$genPredicate$1(basicPhysicalOperators.scala:139)\n\tat org.apache.spark.sql.execution.FilterExec$$anonfun$13.apply(basicPhysicalOperators.scala:179)\n\tat org.apache.spark.sql.execution.FilterExec$$anonfun$13.apply(basicPhysicalOperators.scala:163)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:163)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.produceRows(ColumnarBatchScan.scala:172)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:85)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:125)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:654)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:166)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:40)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 39 more\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT title, COUNT(*), MIN(rank) as peak\n",
    "            FROM test\n",
    "            WHERE artist = \"ed sheeran\" AND MIN(rank) < 6\n",
    "            GROUP BY 1\n",
    "            ORDER BY 2 DESC\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT title)|\n",
      "+---------------------+\n",
      "|                   26|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "            SELECT COUNT(DISTINCT title)\n",
    "            FROM test\n",
    "            WHERE artist = \"ed sheeran\"\n",
    "            \n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
